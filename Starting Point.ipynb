{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library and Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import TWB.xliff as xliff\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import queue\n",
    "import numpy as np\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize unicode characters\n",
    "def NFD(s):\n",
    "        return unicodedata.normalize('NFD', s)\n",
    "\n",
    "# put your data path here\n",
    "data_dir = '/home/mixalis/Downloads/Translators Without Borders/'\n",
    "metadata = pd.read_excel(data_dir + 'Hackathon-for-Good-2019_TWB-Challenge_Metadata.xlsx')\n",
    "\n",
    "# choose only text documents\n",
    "# TODO The metadata contain some files multiple times and the table is almost 3 times the number of documents\n",
    "# Using various combinations of subsets of the columns to get all the 12156 documents did not succeed\n",
    "accepted_documents = metadata.loc[(metadata['Format'] == 'doc') | (metadata['Format'] == 'pdf')].drop_duplicates()\n",
    "\n",
    "# fix some problems with the encoding of special characters in filenames\n",
    "accepted_documents['Filename'] = accepted_documents['Filename'].apply(NFD)\n",
    "\n",
    "# update data path with the sdlxliff directory\n",
    "data_dir += 'hackathon-for-good-2019_TWB-challenge_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all sdlxliff filenames into a list\n",
    "document_names = []\n",
    "\n",
    "# trying to get all doc documents based on extensions\n",
    "for document in os.listdir(data_dir):    \n",
    "    if fnmatch.fnmatch(document,'*.sdlxliff'):\n",
    "        if fnmatch.fnmatch(document,'*.doc*'):\n",
    "            document_names.append(document)\n",
    "        elif fnmatch.fnmatch(document,'*.DOC*'):\n",
    "            document_names.append(document)\n",
    "        elif fnmatch.fnmatch(document,'*.txt*'):\n",
    "            document_names.append(document)    \n",
    "        elif fnmatch.fnmatch(document,'*.pdf*'):\n",
    "            document_names.append(document)\n",
    "        elif fnmatch.fnmatch(document,'*.PDF*'):\n",
    "            document_names.append(document)\n",
    "        elif fnmatch.fnmatch(document,'*.odt*'):\n",
    "            document_names.append(document)\n",
    "        elif fnmatch.fnmatch(document,'*.rtf*'):\n",
    "            document_names.append(document)\n",
    "        elif fnmatch.fnmatch(document,'*.dotx*'):\n",
    "            document_names.append(document)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the translated contents of all documents\n",
    "document_contents = []\n",
    "document_source_langs = []\n",
    "document_target_langs = []\n",
    "\n",
    "for i in range(len(document_names)):\n",
    "    document = document_names[i]\n",
    "    temp_xliff = xliff.XLIFF(data_dir + document)\n",
    "    document_contents.append(temp_xliff.target)\n",
    "    document_source_langs.append(temp_xliff.source_lang)\n",
    "    document_target_langs.append(temp_xliff.target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same unicode fix as for accepted_documents\n",
    "for i in range(len(document_names)):\n",
    "    document_names[i] = NFD(document_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the sdlxliff extension as most files in the metadata are without it\n",
    "filenames = []\n",
    "for i in range(len(document_names)):\n",
    "    filenames.append(document_names[i].replace('.sdlxliff', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2733\n"
     ]
    }
   ],
   "source": [
    "# find the documents that most probably have no metadata, they are a lot\n",
    "documents_without_metadata = []\n",
    "\n",
    "count = 0\n",
    "for name in filenames:\n",
    "    temp = accepted_documents.loc[accepted_documents['Filename'] == name]\n",
    "    if temp.empty:\n",
    "        # for the special case that the translated version's extension is used\n",
    "        temp2 = accepted_documents.loc[accepted_documents['Filename'] == name + '.sdlxliff']\n",
    "        if temp2.empty:\n",
    "            count += 1       \n",
    "            documents_without_metadata.append(name)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø·Ø¹Ø§Ù_x0085_Ù_x0083_Ù_x0085__Ø§Ù_x0084_Ù_x008a_Ù_x0088_Ù_x0085_.docx\n",
      "å_x0080__x008b_äººè_x0087_ªå_x0082_³.docx\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# find the few entries in the metadata that are not in the documents \n",
    "# and the corresponding document filenames for the metadata filenames with wrong encoding\n",
    "import difflib\n",
    "\n",
    "filename_correspondence = {}\n",
    "count = 0\n",
    "for name in list(accepted_documents['Filename'].drop_duplicates()):\n",
    "    if name not in filenames and name not in document_names:\n",
    "        max_similarity = 0.0\n",
    "        corresponding_document = None\n",
    "        for filename in filenames:\n",
    "            # filename letter similarity\n",
    "            seq = difflib.SequenceMatcher(None, filename, name)\n",
    "            if (seq.ratio() > max_similarity):\n",
    "                max_similarity = seq.ratio()\n",
    "                corresponding_document = filename\n",
    "        # this threshold was manually checked and it produces only one false positive\n",
    "        if max_similarity > 0.5:\n",
    "            filename_correspondence[name] = corresponding_document\n",
    "        else:\n",
    "            count += 1\n",
    "            print(name)\n",
    "\n",
    "# delete the false positive \n",
    "del filename_correspondence['Patient_Release_Form_-_Final_4.25.docx']                \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that changes the wrongly encoded metadata filenames\n",
    "def change_names(x):\n",
    "    if x in filename_correspondence.keys():\n",
    "        return filename_correspondence[x]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# start combining all present features/data per document\n",
    "all_document_data = accepted_documents\n",
    "all_document_data['Filename'] = accepted_documents['Filename'].apply(lambda x: change_names(x))\n",
    "\n",
    "# combine document lines into one list instead of a list of lists for use in a DataFrame column\n",
    "contents = []\n",
    "for i in range(len(document_contents)):\n",
    "    temp = ''\n",
    "    for j in range(len(document_contents[i])):\n",
    "        temp += document_contents[i][j] + ' '\n",
    "    contents.append(temp)\n",
    "\n",
    "# left inner join of DataFrames\n",
    "# all_document_data contains information on all documents with content that have also metadata\n",
    "name_contents = pd.DataFrame(columns=['Filename','Content'])\n",
    "name_contents['Filename'] = filenames\n",
    "name_contents['Content'] = contents\n",
    "all_document_data = pd.merge(all_document_data,name_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify which documents are missing from the previous DataFrame\n",
    "s1 = set(all_document_data['Filename'])\n",
    "s2 = set(name_contents['Filename'])\n",
    "no_metadata = list(s2.difference(s1))\n",
    "\n",
    "# create another DataFrame that contains only the filename, \n",
    "# source language, target language and content of these documents\n",
    "no_metadata_source_langs = []\n",
    "no_metadata_target_langs = []\n",
    "no_metadata_contents = []\n",
    "\n",
    "for i in no_metadata:\n",
    "    j = filenames.index(i)\n",
    "    no_metadata_source_langs.append(document_source_langs[j])\n",
    "    no_metadata_target_langs.append(document_target_langs[j])\n",
    "    no_metadata_contents.append(contents[j])\n",
    "\n",
    "no_metadata_df = pd.DataFrame(columns=['Filename', 'Source_lang', 'Target_lang', 'Content'])\n",
    "no_metadata_df['Source_lang'] = no_metadata_source_langs\n",
    "no_metadata_df['Target_lang'] = no_metadata_target_langs\n",
    "no_metadata_df['Content'] = no_metadata_contents\n",
    "no_metadata_df['Filename'] = no_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
