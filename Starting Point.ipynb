{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library and Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import TWB.xliff as xliff\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import queue\n",
    "import numpy as np\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize unicode characters\n",
    "def NFD(s):\n",
    "        return unicodedata.normalize('NFD', s)\n",
    "\n",
    "# put your data path here\n",
    "data_dir = ''\n",
    "metadata = pd.read_excel(data_dir + 'Hackathon-for-Good-2019_TWB-Challenge_Metadata.xlsx')\n",
    "\n",
    "# choose only text documents\n",
    "# TODO The metadata contain some files multiple times and the table is almost 3 times the number of documents\n",
    "# Using various combinations of subsets of the columns to get all the 12156 documents did not succeed\n",
    "accepted_documents = metadata.loc[metadata['Format'] == 'doc'].drop_duplicates()\n",
    "\n",
    "# fix some problems with the encoding of special characters in filenames\n",
    "accepted_documents['Filename'] = accepted_documents['Filename'].apply(NFD)\n",
    "\n",
    "# update data path with the sdlxliff directory\n",
    "data_dir += 'hackathon-for-good-2019_TWB-challenge_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all sdlxliff filenames into a list\n",
    "document_names = []\n",
    "\n",
    "# trying to get all doc documents based on extensions\n",
    "for document in os.listdir(data_dir):    \n",
    "    if fnmatch.fnmatch(document,'*.sdlxliff'):\n",
    "        if fnmatch.fnmatch(document,'*.doc*'):\n",
    "            document_names.append(document)\n",
    "        if fnmatch.fnmatch(document,'*.DOC*'):\n",
    "            document_names.append(document)\n",
    "        if fnmatch.fnmatch(document,'*.txt*'):\n",
    "            document_names.append(document)    \n",
    "        if fnmatch.fnmatch(document,'*.pdf*'):\n",
    "            document_names.append(document)\n",
    "        if fnmatch.fnmatch(document,'*.PDF*'):\n",
    "            document_names.append(document)\n",
    "        if fnmatch.fnmatch(document,'*.odt*'):\n",
    "            document_names.append(document)\n",
    "        if fnmatch.fnmatch(document,'*.rtf*'):\n",
    "            document_names.append(document)\n",
    "        if fnmatch.fnmatch(document,'*.dotx*'):\n",
    "            document_names.append(document)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the translated contents of all documents\n",
    "document_contents = []\n",
    "\n",
    "for document in list(document_names):\n",
    "    # Avoid a couple of (probably) malformed xml documents(No root found by the parser)\n",
    "    try:\n",
    "        document_contents.append(xliff.XLIFF(data_dir + document).target)\n",
    "    except AttributeError:\n",
    "        print(document)\n",
    "        document_names.remove(document)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same unicode fix as for accepted_documents\n",
    "for i in range(len(document_names)):\n",
    "    document_names[i] = NFD(document_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the sdlxliff extension as most files in the metadata are without it\n",
    "filenames = []\n",
    "for name in document_names:\n",
    "    filenames.append(name.replace('.sdlxliff', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2788\n"
     ]
    }
   ],
   "source": [
    "# find the documents that most probably have no metadata, they are a lot\n",
    "documents_without_metadata = []\n",
    "\n",
    "count = 0\n",
    "for name in filenames:\n",
    "    temp = accepted_documents.loc[accepted_documents['Filename'] == name]\n",
    "    if temp.empty:\n",
    "        # for the special case that the translated version's extension is used\n",
    "        temp2 = accepted_documents.loc[accepted_documents['Filename'] == name + '.sdlxliff']\n",
    "        if temp2.empty:\n",
    "            count += 1\n",
    "        documents_without_metadata.append(name)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø·Ø¹Ø§Ù_x0085_Ù_x0083_Ù_x0085__Ø§Ù_x0084_Ù_x008a_Ù_x0088_Ù_x0085_.docx\n",
      "å_x0080__x008b_äººè_x0087_ªå_x0082_³.docx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the few entries in the metadata that are not in the documents \n",
    "# and the corresponding document filenames for the metadata filenames with weird special characters\n",
    "import difflib\n",
    "\n",
    "filename_correspondence = {}\n",
    "count = 0\n",
    "for name in list(accepted_documents['Filename'].drop_duplicates()):\n",
    "    if name not in filenames and name not in document_names:\n",
    "        max_similarity = 0.0\n",
    "        corresponding_document = None\n",
    "        for filename in filenames:\n",
    "            # filename letter similarity\n",
    "            seq = difflib.SequenceMatcher(None, filename, name)\n",
    "            if (seq.ratio() > max_similarity):\n",
    "                max_similarity = seq.ratio()\n",
    "                corresponding_document = filename\n",
    "        # this threshold was manually checked and it produces only one false positive\n",
    "        if max_similarity > 0.5:\n",
    "            filename_correspondence[name] = corresponding_document\n",
    "        else:\n",
    "            count += 1\n",
    "            print(name)\n",
    "\n",
    "# delete the false positive \n",
    "del filename_correspondence['Patient_Release_Form_-_Final_4.25.docx']                \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
