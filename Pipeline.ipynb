{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movie reviews dataset\n",
    "data = pd.read_csv('data/documents', sep='\\t', header=None).apply(lambda x: x[0].strip(), axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the parser\n",
    "\n",
    "Make sure that the coreNLP server is running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import spacy\n",
    "\n",
    "def is_token_interesting(token):\n",
    "    \"\"\"\n",
    "    How do we define a token as interesting?\n",
    "    In this function, We define it as interesting if it satisfies the following conditions\n",
    "    * It is not a stopword\n",
    "    * It is not identical to its head word in the grammatical structure\n",
    "    * Both it, AND the head word are verbs or nouns.\n",
    "        * i.e. we are not interested in adverbs, but we are specifically interested in verbs that act on nouns, or vice versa.\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    token: spacy.tokens.token.Token\n",
    "        The token to evaluate\n",
    "\n",
    "    Output:\n",
    "        Boolean\n",
    "    \"\"\"\n",
    "    if len(token) < 2:\n",
    "        return False\n",
    "    if token.is_stop:\n",
    "        return False\n",
    "    elif token == token.head:\n",
    "        return False\n",
    "    elif (token.pos_ in [ 'NOUN', 'VERB']):\n",
    "        return True\n",
    "    #fi\n",
    "    return False\n",
    "#edef\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "def gen_tokensets(documents, func=is_token_interesting, max_sents=None, nlp=None):\n",
    "    \"\"\"\n",
    "    Generate the tokensets for a given document.\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    document: List[String] | String.\n",
    "        The document(s) in a string format\n",
    "        \n",
    "    func: Function(spacy.tokens.token.Token) -> Boolean\n",
    "        Decide whether to keep the token or not\n",
    "        \n",
    "    max_sents: None | Integer\n",
    "        The maximum number of sentences to use. None implies no limit.\n",
    "        \n",
    "    nlp: None | String | spacy.lang.*\n",
    "        The Spacy language model\n",
    "        None -> Use the english language model\n",
    "        String -> Load a specific language model\n",
    "        spacy.lang.* -> Use a pre-loaded language model\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        List[List[set[String]]]\n",
    "    \"\"\"\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm') if nlp is None else spacy.load(nlp) if isinstance(nlp, str) else nlp\n",
    "    \n",
    "    def gen_tokensets_single(document, func, max_sents):\n",
    "        doc = nlp(document)\n",
    "        S = []\n",
    "        for i, s in enumerate(doc.sents):\n",
    "            S.append(set([t.lemma_ for t in s if func(t) ]))\n",
    "            if i > max_sents:\n",
    "                break\n",
    "            #fi\n",
    "        #efor\n",
    "        return S\n",
    "    #edef\n",
    "\n",
    "    if max_sents is None:\n",
    "        max_sents = np.inf\n",
    "    #fi\n",
    "\n",
    "    if isinstance(documents, str):\n",
    "        documents = [ documents ]\n",
    "    #fi\n",
    "\n",
    "    T = []\n",
    "    for i, d in enumerate(documents):\n",
    "        print(\"\\r%d/%s\" % (i+1, str(len(documents)) if hasattr(documents, '__len__') else '?'), end='')\n",
    "        T.append(gen_tokensets_single(d, func, max_sents))\n",
    "    #efor\n",
    "    print()\n",
    "\n",
    "    return T\n",
    "#edef\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "def network_from_tokensets(tokensets, min_docs=4, min_tokens=1):\n",
    "    \"\"\"\n",
    "    Given a list of sets of interesting tokens, generate a pandas dataframe of setsXtokens, indicating presents/absense\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    sets: List[List[Set[str]]]\n",
    "        For each document, a list of sets (each which represents a sentence) of tokens.\n",
    "\n",
    "    min_docs: Integer\n",
    "        The minimum number of documents required to retain a token in the network (default 4)\n",
    "\n",
    "    min_tokens: Integer\n",
    "        The minimum number of tokens required to retain a sentence in the network (default 1)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with 2+n_tokens columns and n_sentences rows.\n",
    "        The first two columns, `document_` and `sentence_` represent the input indexes of the corresponding\n",
    "        document and sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq  = list(set.union(*[ set(s) for d in tokensets for s in d ]))\n",
    "    uniqa = np.array(uniq)\n",
    "    \n",
    "    # A mapping from tokens to integers Offset of 2 for doc and sentence ID\n",
    "    M = { t:i+2 for i,t in enumerate(uniq) }\n",
    "    \n",
    "    # Generate a matrix of zero counts\n",
    "    # The first two columns are for the document and sentence IDs\n",
    "    P = np.zeros((sum([len(s) for s in tokensets]), 2+len(uniq)))\n",
    "    \n",
    "    # Fill the first two columns with document and sentence IDs\n",
    "    P[:,[0,1]] = [ [i,j] for i in range(len(tokensets)) for j in range(len(tokensets[i])) ]\n",
    "    \n",
    "    # Fill the matrix, looping over the sets\n",
    "    k = 0\n",
    "    for i, doc in enumerate(tokensets):\n",
    "        for j, s in enumerate(doc):\n",
    "            tok_ids = [ M[t] for t in s ]\n",
    "            P[k,tok_ids] = 1\n",
    "            k = k+1\n",
    "        #efor\n",
    "    #efor\n",
    "    \n",
    "    meta = ['document_', 'sentence_']\n",
    "    N = pd.DataFrame(P, columns=meta + uniq)\n",
    "    rel_uniq = list(N.columns[2:][(N.drop(columns='sentence_').groupby('document_').agg(sum) > 0).sum() > min_docs])\n",
    "    N = N[meta + rel_uniq]\n",
    "    N = N.loc[N[rel_uniq].sum(axis=1) >= min_tokens]\n",
    "    return N\n",
    "#edef\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "def is_network(N):\n",
    "    \"\"\"\n",
    "    Check if a pandas dataframe is a valid network.\n",
    "    Checks if `document_` and `sentence_` are in column positions 0 and 1, respectively.\n",
    "    Raises error if not.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    N: pandas.DataFrame\n",
    "        A pandas dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    network: Boolean\n",
    "        Is the dataframe a network?\n",
    "    \"\"\"\n",
    "    return (N.columns[0] == 'document_') and (N.columns[1] == 'sentence_')\n",
    "#edef\n",
    "\n",
    "def is_annotation(A):\n",
    "    \"\"\"\n",
    "    Check if a pandas dataframe is a valid annotation.\n",
    "    Checks if `document_` is in column position 0.\n",
    "    Raises error if not.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    A: pandas.DataFrame\n",
    "        A pandas dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    network: Boolean\n",
    "        Is the dataframe an annotation?\n",
    "    \"\"\"\n",
    "    return (A.columns[0] == 'document_')\n",
    "#edef\n",
    "\n",
    "def _meta(NA):\n",
    "    \"\"\"\n",
    "    Returns the metadata of a network or an annotation\n",
    "    Assumes the network/annotation is already validated\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    NA: pandas.DataFrame\n",
    "        A Network or an Annotation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas dataframe with only the metadata\n",
    "    \"\"\"\n",
    "    idx_offset = 1 + (NA.columns[1] == 'sentence_')\n",
    "    return NA[NA.columns[:idx_offset]]\n",
    "#edef\n",
    "\n",
    "def _features(NA):\n",
    "    \"\"\"\n",
    "    Returns the features of a network or an annotation\n",
    "    Assumes the network/annotation is already validated\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    NA: pandas.DataFrame\n",
    "        A Network or an Annotation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas dataframe with only the features\n",
    "    \"\"\"\n",
    "    idx_offset = 1 + (NA.columns[1] == 'sentence_')\n",
    "    return NA[NA.columns[idx_offset:]]\n",
    "#edef\n",
    "\n",
    "\n",
    "def validate(NA, network=None):\n",
    "    \"\"\"\n",
    "    Validate a dataframe as either a Network or an Annotation\n",
    "    Raises error if not the case.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    NA: pd.DataFrame\n",
    "        The network or annotation dataframe to test\n",
    "    network: None | True | False\n",
    "        None -> Test if NA is a network OR an annotation\n",
    "        True -> Test if NA is a network\n",
    "        False -> Test if NA is an annotation\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    network: Boolean\n",
    "        True if NA is a network\n",
    "        False if NA is an annotation\n",
    "        \n",
    "    Raises\n",
    "    \"\"\"\n",
    "    \n",
    "    is_n = is_network(NA)\n",
    "    is_a = is_annotation(NA)\n",
    "    \n",
    "    \n",
    "    if network is None:\n",
    "        if not (is_n or is_a):\n",
    "            raise ValueError(\"The object is not a network or an anootation.\")\n",
    "        else:\n",
    "            return is_n\n",
    "        #fi\n",
    "    #fi\n",
    "    \n",
    "    if network:\n",
    "        if not is_n:\n",
    "            raise ValueError(\"The object is not a network.\")\n",
    "        #fi\n",
    "        return True\n",
    "    else:\n",
    "        if not is_a:\n",
    "            raise ValueError(\"The object is not an annotation.\")\n",
    "        #fi\n",
    "        return False\n",
    "    #fi\n",
    "#edef\n",
    "        \n",
    "def is_collapsed(A):\n",
    "    \"\"\"\n",
    "    Is an annotation collapsed?\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    A: pandas.DataFrame\n",
    "        A pandas dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    collapsed: Boolean\n",
    "        Is the dataframe collapsed? (no 'sentence_' column in position 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    validate(A, False)\n",
    "    \n",
    "    return not (('document_' == A.columns[0]) and ('sentence_' == 'A.columns[1]'))\n",
    "#edef\n",
    "        \n",
    "    \n",
    "\n",
    "def feature_type(NA):\n",
    "    \"\"\"\n",
    "    Return the type of the features encoded in the dataframe\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    NA: pandas.DataFrame\n",
    "        A Network or an Annotation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tokens: Boolean\n",
    "        Are the features in this network tokens?\n",
    "        True if it is the case.\n",
    "        False if they are concepts\n",
    "    \"\"\"\n",
    "    \n",
    "    validate(NA)\n",
    "        \n",
    "    return not ('_' in ''.join(NA.columns[1 + (1 if ('sentence_' == NA.columns[1]) else 0):]))\n",
    "#edef\n",
    "\n",
    "def collapse(A):\n",
    "    \"\"\"\n",
    "    Collapse all sentences into a single document.\n",
    "    The counts of features (tokens/concepts) are summed up across sentences within a document\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    A: pandas.DataFrame\n",
    "        Dataframe of feature annotations, with at least columns `document_` and `_sentences`.\n",
    "\n",
    "    Returns:\n",
    "    C: pandas.DataFrame\n",
    "        The annotations with counts across sentences within a sentence summed up.\n",
    "        The `sentences_` column is removed.\n",
    "    \"\"\"\n",
    "\n",
    "    if is_collapsed(A):\n",
    "        return A.copy()\n",
    "    #fi\n",
    "\n",
    "    return self._obj.drop(columns='sentence_').groupby('document_').agg(sum).reset_index()\n",
    "#edef\n",
    "\n",
    "def extract_concepts(N, method, *pargs, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract concepts from the network.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    N: pd.DataFrame\n",
    "        The network to extract concepts from\n",
    "    method: String\n",
    "        The method to extract concepts from\n",
    "    *pargs, **kwargs:\n",
    "        Additional arguments for method\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Set[String]]\n",
    "        Each set represents a concept\n",
    "    \"\"\"\n",
    "    \n",
    "    validate(N, True)\n",
    "#edef\n",
    "\n",
    "def idf(N):\n",
    "    \"\"\"\n",
    "    Calculate the IDF of tokens in a given corpus\n",
    "    parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    validate(N)\n",
    "    \n",
    "    X = _features(collapse(N))\n",
    "    return np.log(X.shape[0] / (1+(X>1).sum()))\n",
    "#edef\n",
    "\n",
    "def tfidf(N):\n",
    "    \"\"\"\n",
    "    Calculate the TFIDF for a given corpus.\n",
    "    \"\"\"\n",
    "    X = _features(N.collapse())\n",
    "    idf = np.log(X.shape[0] / (1+(X>1).sum()))\n",
    "    return X.divide(X.sum(axis=1), axis=0) * idf\n",
    "#edef\n",
    "\n",
    "def annotate_tokens(N, tokensets):\n",
    "    \"\"\"\n",
    "    Annotate tokensets given a set of tokens that are defined to be interesting\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    N: pd.DataFrame\n",
    "        The network with relevant tokens\n",
    "    tokensets: List[List[Set[String]]]\n",
    "        Tokensets generated for a set of documents with gen_tokensets\n",
    "\n",
    "    Returns:\n",
    "        DataFrame that can be used with the `x.twb_accessors`\n",
    "    \"\"\"\n",
    "    uniqs = set(_features(N).columns)\n",
    "    uniq  = list(uniqs)\n",
    "    uniqa = np.array(uniq)\n",
    "\n",
    "    # A mapping from tokens to integers Offset of 2 for doc and sentence ID\n",
    "    M = { t:i+2 for i,t in enumerate(uniq) }\n",
    "\n",
    "    # Generate a matrix of zero counts\n",
    "    # The first two columns are for the document and sentence IDs\n",
    "    P = np.zeros((sum([len(s) for s in tokensets]), 2+len(uniq)))\n",
    "\n",
    "    # Fill the first two columns with document and sentence IDs\n",
    "    P[:,[0,1]] = [ [i,j] for i in range(len(tokensets)) for j in range(len(tokensets[i])) ]\n",
    "\n",
    "    # Fill the matrix, looping over the sets\n",
    "    k = 0\n",
    "    for i, doc in enumerate(tokensets):\n",
    "        for j, s in enumerate(doc):\n",
    "            tok_ids = [ M[t] for t in (set(s) & uniqs)  ]\n",
    "            P[k,tok_ids] = 1\n",
    "            k = k+1\n",
    "        #efor\n",
    "    #efor\n",
    "\n",
    "    meta = list(_meta(N).columns)\n",
    "\n",
    "    F = pd.DataFrame(P, columns=meta + uniq)\n",
    "    return F\n",
    "#edef\n",
    "\n",
    "def annotate_concepts(N, tokensets):\n",
    "    \"\"\"\n",
    "    Annotate tokensets given a set of concepts that are defined to be interesting\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    N: pd.DataFrame\n",
    "        The network with relevant concepts\n",
    "    tokensets: List[List[Set[String]]]\n",
    "        Tokensets, produced by gen_tokensets\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    A: pd.DataFrame\n",
    "        An annotation of concepts per sentence\n",
    "    \"\"\"\n",
    "    concepts = [ set(x.split('_')) for x in _features(N).columns ]\n",
    "    uniq     = set.union(*concepts)\n",
    "\n",
    "    P = np.zeros((sum([len(s) for s in tokensets]), 2+len(concepts)))\n",
    "\n",
    "    # Fill the first two columns with document and sentence IDs\n",
    "    P[:,[0,1]] = [ [i,j] for i in range(len(tokensets)) for j in range(len(tokensets[i])) ]\n",
    "\n",
    "    # Fill the matrix, looping over the sets\n",
    "    k = 0\n",
    "    for i, doc in enumerate(tokensets):\n",
    "        for j, s in enumerate(doc):\n",
    "            s = set(s) & uniq\n",
    "            concept_ids = [ c.issubset(s) for c in concepts ]\n",
    "            P[k,concept_ids] = 1\n",
    "            k = k+1\n",
    "        #efor\n",
    "    #efor\n",
    "\n",
    "    meta = list(_meta(N).columns)\n",
    "    F = pd.DataFrame(P, columns=meta + ['_'.join(sorted(list(c))) for c in concepts])\n",
    "    return F\n",
    "#edef\n",
    "\n",
    "def pdist(A, metric='euclidean', document_metric=None, matrix=True):\n",
    "    \"\"\"\n",
    "    Calculate distances between documents by calculating distance between sentences.\n",
    "\n",
    "    parameters:\n",
    "    -----------\n",
    "    annotation: pd.DataFrame\n",
    "        The result of an annotation\n",
    "\n",
    "    metric: None | String | 'poscorr'\n",
    "        The distance metric to use between sentences. See fastcluster.pdist\n",
    "        poscorr is positive correlation\n",
    "\n",
    "    merge_dist: None | min | max | mean | median\n",
    "        How to calculate distances between documents, on the basis of the distances between sentences\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    Y: ndarray\n",
    "        Returns a condensed distance matrix Y.  For\n",
    "        each :math:`i` and :math:`j` (where :math:`i<j<m`),where m is the number\n",
    "        of original observations. The metric ``dist(u=X[i], v=X[j])``\n",
    "        is computed and stored in entry ``ij``.\n",
    "    \"\"\"\n",
    "    #Remove sentences with no annotations\n",
    "    validate(A, False)\n",
    "\n",
    "    O_doc_idx = A.document_.copy()\n",
    "    N_docs = A.document_.unique().shape[0]\n",
    "    A = A[_features(A).sum(axis=1) > 0]\n",
    "    D_doc_idx = A.document_\n",
    "\n",
    "    if metric == 'poscorr':\n",
    "        D_S = np.corrcoef(_features(A).values)\n",
    "        D_S[D_S < 0] = 0\n",
    "        D_S = 1 - D_S\n",
    "    else:\n",
    "        D_S = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(_features(A), metric=metric))\n",
    "    #fi\n",
    "    \n",
    "    document_metric = {'min':np.min, 'max':np.max,\n",
    "                       'mean':np.mean, None: np.min,\n",
    "                       'median':np.median}[document_metric]\n",
    "\n",
    "    D_D = np.zeros((N_docs, N_docs))\n",
    "    for i in range(N_docs):\n",
    "        for j in range(i, N_docs):\n",
    "            if i == j:\n",
    "                D_D[i,j] = 0\n",
    "            else:\n",
    "                rm = D_S[D_doc_idx == i,:][:,D_doc_idx == j]\n",
    "                D_D[i,j] = document_metric(rm) if np.prod(rm.shape) > 0 else np.inf\n",
    "                D_D[j,i] = D_D[i,j]\n",
    "            #fi\n",
    "        #efor\n",
    "    #efor\n",
    "    return D_D if matrix else scipy.spatial.distance.squareform(D_D)\n",
    "#edef\n",
    "\n",
    "def pagerank(D, eps=1.0e-8, d=0.85):\n",
    "    \"\"\"\n",
    "    The PageRank Algorithm (taken from wikipedia)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    D: pd.DataFrame\n",
    "        adjacency matrix where M_i,j represents the link from 'j' to 'i', such that for all 'j'\n",
    "        sum(i, M_i,j) = 1\n",
    "\n",
    "    d: Float\n",
    "        damping factor (default value 0.85)\n",
    "\n",
    "    eps: Float\n",
    "        quadratic error for v (default value 1.0e-8)\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "        A vector of ranks such that v_i is the i-th rank from [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    N = D.shape[1]\n",
    "    v = np.random.rand(N, 1)\n",
    "    v = v / np.linalg.norm(v, 1)\n",
    "    last_v = np.ones((N, 1), dtype=np.float32) * 100\n",
    "\n",
    "    while np.linalg.norm(v - last_v, 2) > eps:\n",
    "        last_v = v\n",
    "        v = d * np.matmul(D, v) + (1 - d) / N\n",
    "    #ewhile\n",
    "    return v\n",
    "#edef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T = gen_tokensets(list(data[:100]) + list(data[-100:]))\n",
    "N = network_from_tokensets(T, min_docs=1)\n",
    "A = annotate_tokens(N, T)\n",
    "D = pdist(collapse(A), metric='poscorr')\n",
    "ND = D / D.sum(axis=1)\n",
    "PR = pagerank(ND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.003183865725603286, 0.00703819034334156)"
      ]
     },
     "execution_count": 941,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
