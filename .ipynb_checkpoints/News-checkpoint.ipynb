{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "url = ('https://newsapi.org/v2/everything?'\n",
    "       #'category=general&'\n",
    "       'domains=bbc.co.uk&'\n",
    "       #'country=gb&'\n",
    "       'pageSize=99&'\n",
    "       'page=1&'\n",
    "       'from=2019-05-05&to=2019-06-03&'\n",
    "       'apiKey=34e73062364d41b180a6f6a9625bff79')\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('data/data.json', 'w') as outfile:\n",
    "    json.dump(response.json(), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_df = pd.DataFrame(columns=['Title', 'Date', 'Stems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data/data.json\") as f:\n",
    "        loaded_json = json.load(f)\n",
    "        for i in range(len(loaded_json[\"articles\"])):\n",
    "            if loaded_json[\"articles\"][i][\"content\"] is not None:\n",
    "                news_df = news_df.append({'Title': loaded_json[\"articles\"][i][\"title\"], 'Date': loaded_json[\"articles\"][i][\"publishedAt\"]}, ignore_index=True)\n",
    "                #print(loaded_json[\"articles\"][i][\"title\"])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy import displacy\n",
    "# from collections import Counter\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def stemSentence(sentence):\n",
    "    token_words=tokenizer.tokenize(sentence)\n",
    "    token_words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #print(stop_words)\n",
    "    filter_sentense = [w for w in token_words if not w in stop_words]\n",
    "    \n",
    "    stem_sentence=[]\n",
    "    for word in filter_sentense:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=news_df.Title[0]\n",
    "x=stemSentence(sentence)\n",
    "print(sentence)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in news_df.index:\n",
    "    news_df.Stems[i] = stemSentence(news_df.Title[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = []\n",
    "master_wordlist = []\n",
    "for i in news_df.index:\n",
    "    wordlist = news_df.Stems[i].split()\n",
    "\n",
    "    master_wordlist+= wordlist\n",
    "#print(master_wordlist)\n",
    "# print(set(master_wordlist))\n",
    "# for w in master_wordlist:\n",
    "#     wordfreq.append(master_wordlist.count(w))\n",
    "\n",
    "# ww = list(zip(master_wordlist, wordfreq))\n",
    "# print(\"Pairs\\n\" + str(ww[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(zip(wordlist,wordfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = wordListToFreqDict(master_wordlist)\n",
    "# print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in word_freq.items():\n",
    "#     print(x, y)\n",
    "# print(word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = list(word_freq.keys())\n",
    "counts = list(word_freq.values())\n",
    "\n",
    "# sorting the bars means sorting the range factors\n",
    "sorted_fruits = sorted(fruits, key=lambda x: counts[fruits.index(x)])\n",
    "\n",
    "p = figure(x_range=sorted_fruits, plot_height=500, plot_width=1400, title=\"Word frequencies\",\n",
    "           toolbar_location=\"below\")\n",
    "\n",
    "p.vbar(x=fruits, top=counts, width=0.9)\n",
    "p.xaxis.major_label_orientation = \"vertical\"\n",
    "p.xgrid.grid_line_color = None\n",
    "p.xaxis.axis_label_text_font_size = \"25pt\"\n",
    "p.xaxis.major_label_text_font_size = \"10pt\"\n",
    "p.y_range.start = 0\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google trends (hourly data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import os  \n",
    "import numpy as np\n",
    "pytrend = TrendReq(hl='en-US', tz=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def get_interest_and_region(kw_list,pytrend):\n",
    "    if not (os.path.isfile('Data/'+str(kw_list[0])+'.pkl') and os.path.isfile('Data/'+str(kw_list[0])+'_regions'+'.pkl')):\n",
    "        print('Term not downloaded yet, downloading now..')\n",
    "        \n",
    "        #pytrend = TrendReq(hl='en-US', tz=360, timeout=(10,25), proxies=['https://34.203.233.13:80',], retries=5, backoff_factor=1)\n",
    "\n",
    "        search_df = pytrend.get_historical_interest(kw_list, year_start=2005, year_end=2018).reset_index()\n",
    "\n",
    "        search_df.to_pickle('Data/'+str(kw_list[0])+'.pkl')\n",
    "        region_df = pytrend.interest_by_region(resolution='COUNTRY', inc_low_vol=False, inc_geo_code=False)\n",
    "        region_df.to_pickle('Data/'+str(kw_list[0])+'_regions'+'.pkl')\n",
    "        print('term with regional data saved')\n",
    "        return(search_df, region_df)\n",
    "    else:\n",
    "        print('term has been found..')\n",
    "        pytrend = TrendReq(hl='en-US', tz=360)   \n",
    "        term_df = pd.read_pickle('Data/'+str(kw_list[0])+'.pkl')\n",
    "        region_df = pd.read_pickle('Data/'+str(kw_list[0])+'_regions'+'.pkl')\n",
    "        print(term_df.head())\n",
    "        return(term_df, region_df)\n",
    "        \n",
    "        \n",
    "def plot_interest_in_time(df2, kw_list):\n",
    "\n",
    "    from bokeh.io import show, output_file\n",
    "    from bokeh.plotting import figure\n",
    "    from bokeh.io import output_notebook\n",
    "    from datetime import datetime as dt\n",
    "    from bokeh.models import DatetimeTickFormatter\n",
    "\n",
    "    output_notebook()\n",
    "    TOOLS = 'save,pan,box_zoom,reset,wheel_zoom,hover'\n",
    "    TOOLTIPS = [\n",
    "        ('month', '@x{datetime}'),\n",
    "        ('Total searches', '@y'),\n",
    "    ]\n",
    "\n",
    "    p = figure(title=\"Monthly-wise total search of term - \"+str(kw_list[0]), y_axis_type=\"linear\", plot_height = 400,\n",
    "               tools = TOOLS, tooltips = TOOLTIPS, plot_width = 800)\n",
    "\n",
    "\n",
    "    p.xaxis.axis_label = 'Month'\n",
    "    p.yaxis.axis_label = 'Total search'\n",
    "    #p.circle(2010, temp_df.IncidntNum.min(), size = 10, color = 'red')\n",
    "\n",
    "    p.line(df2.index, df2[str(kw_list[0])],line_color=\"purple\", line_width = 3)\n",
    "\n",
    "    p.xaxis.formatter=DatetimeTickFormatter(\n",
    "\n",
    "            days=[\"%d %B %Y\"],\n",
    "            months=[\"%d %B %Y\"],\n",
    "            years=[\"%d %B %Y\"],\n",
    "        )\n",
    "    # output_file(\"line_chart.html\", title=\"Line Chart\")\n",
    "    show(p)\n",
    "\n",
    "\n",
    "def interest_test(kw_list, pytrend):\n",
    "    #create patload\n",
    "    pytrend.build_payload(kw_list, timeframe='all')\n",
    "    interest_over_time_df = pytrend.interest_over_time()\n",
    "    search_df.to_pickle('Data/trends/'+str(kw_list[0])+'.pkl')\n",
    "    return(interest_over_time_df)\n",
    "    #print(interest_over_time_df)\n",
    "#edef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_list = ['greece crisis']\n",
    "\n",
    "#time_df, regions_df = get_interest_and_region(kw_list, pytrend)\n",
    "\n",
    "df = interest_test(kw_list,pytrend)\n",
    "plot_interest_in_time(df, kw_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downsampling to monthly\n",
    "df2 = time_df.resample('M', on='date').mean()\n",
    "print(df2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interest_in_time(df2,kw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add year and month\n",
    "# term_csv['Year'] = term_csv.date.apply(lambda x: x.year)\n",
    "# term_csv['Month'] = term_csv.date.apply(lambda x: x.month)\n",
    "# term_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = term_csv.groupby(['Year']).mean().reset_index()\n",
    "\n",
    "# temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = pd.read_pickle('Data/crisis_regions'+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google trends (Interest over time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import os  \n",
    "import numpy as np\n",
    "pytrend = TrendReq(hl='en-US', tz=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interest_test(kw_list, pytrend):\n",
    "    #create patload\n",
    "    pytrend.build_payload(kw_list, timeframe='all')\n",
    "    interest_over_time_df = pytrend.interest_over_time()\n",
    "    interest_over_time_df.to_pickle('Data/trends/'+str(kw_list[0].replace(\" \", \"_\"))+'_'+str(kw_list[1].replace(\" \", \"_\")) +'.pkl')\n",
    "    region_df = pytrend.interest_by_region(resolution='COUNTRY', inc_low_vol=False, inc_geo_code=False)\n",
    "    #region_df.to_pickle('Data/trends/'+str(kw_list[0].replace(\" \", \"_\"))+'_regions'+'.pkl')\n",
    "    #return(interest_over_time_df)\n",
    "    #print(interest_over_time_df)\n",
    "#edef\n",
    "def plot_interest_in_time(df2, kw_list):\n",
    "\n",
    "    from bokeh.io import show, output_file\n",
    "    from bokeh.plotting import figure\n",
    "    from bokeh.io import output_notebook\n",
    "    from datetime import datetime as dt\n",
    "    from bokeh.models import DatetimeTickFormatter\n",
    "\n",
    "    output_notebook()\n",
    "    TOOLS = 'save,pan,box_zoom,reset,wheel_zoom,hover'\n",
    "    TOOLTIPS = [\n",
    "        ('month', '@x{datetime}'),\n",
    "        ('Total searches', '@y'),\n",
    "    ]\n",
    "\n",
    "    p = figure(title=\"Monthly-wise total search of term - \"+str(kw_list[0]), y_axis_type=\"linear\", plot_height = 400,\n",
    "               tools = TOOLS, tooltips = TOOLTIPS, plot_width = 800)\n",
    "\n",
    "\n",
    "    p.xaxis.axis_label = 'Month'\n",
    "    p.yaxis.axis_label = 'Total search'\n",
    "    #p.circle(2010, temp_df.IncidntNum.min(), size = 10, color = 'red')\n",
    "\n",
    "    p.line(df2.index, df2[str(kw_list[0])],line_color=\"purple\", line_width = 3)\n",
    "\n",
    "    p.xaxis.formatter=DatetimeTickFormatter(\n",
    "\n",
    "            days=[\"%d %B %Y\"],\n",
    "            months=[\"%d %B %Y\"],\n",
    "            years=[\"%d %B %Y\"],\n",
    "        )\n",
    "    # output_file(\"line_chart.html\", title=\"Line Chart\")\n",
    "    show(p)\n",
    "#edef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to cache countries with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [ 'humanitarian crisis',\n",
    "         'natural disaster',\n",
    "         'environmental crisis',\n",
    "         'disability',\n",
    "         'gender',\n",
    "         'genital mutilation',\n",
    "         'racism',\n",
    "         'genocide',\n",
    "         'civil war',\n",
    "         'terrorism',\n",
    "         'infectious disease',\n",
    "         'political revolution',\n",
    "         'political prisoner',\n",
    "         'amnesty',\n",
    "         'corruption',\n",
    "         'health',\n",
    "         'gender inequality',\n",
    "         'rape',\n",
    "         'ebola',\n",
    "         'aids',\n",
    "         'hiv',\n",
    "         'technology',\n",
    "         'artificial intelligence',\n",
    "         'military',\n",
    "         'war',\n",
    "         'climate change',\n",
    "         'starvation',\n",
    "         'food shortage',\n",
    "         'dehydration',\n",
    "         'water shortage',\n",
    "         'attack',\n",
    "         'aggression',\n",
    "         'logistics',\n",
    "         'nutrition',\n",
    "         'protection',\n",
    "         'shelter',\n",
    "         'drinking water',\n",
    "         'sanitation',\n",
    "         'hygiene',\n",
    "         'refugee camp',\n",
    "         'education',\n",
    "         'emergency communication system',\n",
    "         'food security',\n",
    "         'human rights',\n",
    "         'children',\n",
    "         'pregnancy',\n",
    "         'old age',\n",
    "         'justice',\n",
    "         'homelessness',\n",
    "         'art',\n",
    "         'indigenous peoples'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\n",
    "    'Portugal',\n",
    "    'Russia',\n",
    "    'Myanmar',\n",
    "    'Italy',\n",
    "    'Ireland'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download country and terms\n",
    "region_df = pd.read_pickle('Data/crisis_regions'+'.pkl')\n",
    "for i in range (len(countries)):\n",
    "    for j in range (len(countries)-1):\n",
    "        for term in tags:\n",
    "            #kw_list = [country+' '+term, country+1]\n",
    "    #        interest_test(kw_list, pytrend)\n",
    "            if not (countries[i] == countries[j]):\n",
    "                kw_list = [countries[i] + ' ' +term, countries[j] +' ' +term]\n",
    "                interest_test(kw_list, pytrend)\n",
    "                #print(kw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot an example\n",
    "# ex_df = pd.read_pickle('Data/trends/Algeria_genocide.pkl')\n",
    "# print(ex_df)\n",
    "# kw_li = ['Algeria genocide']\n",
    "# plot_interest_in_time(ex_df, kw_li)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing comnparison of terms\n",
    "# \n",
    "# interest_test(comp_list, pytrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting\n",
    "ex_df = pd.read_pickle('Data/trends/Algeria_aids_Greece_aids.pkl')\n",
    "comp_list = ['Algeria aids', 'Greece aids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook\n",
    "from datetime import datetime as dt\n",
    "from bokeh.models import DatetimeTickFormatter\n",
    "\n",
    "output_notebook()\n",
    "TOOLS = 'save,pan,box_zoom,reset,wheel_zoom,hover'\n",
    "TOOLTIPS = [\n",
    "    ('month', '@x{datetime}'),\n",
    "    ('Total searches', '@y'),\n",
    "]\n",
    "\n",
    "p = figure(title=\"Monthly-wise total search of term - \"+str(comp_list), y_axis_type=\"linear\", plot_height = 400,\n",
    "               tools = TOOLS, tooltips = TOOLTIPS, plot_width = 800)\n",
    "\n",
    "\n",
    "p.xaxis.axis_label = 'Month'\n",
    "p.yaxis.axis_label = 'Total search'\n",
    "#p.circle(2010, temp_df.IncidntNum.min(), size = 10, color = 'red')\n",
    "\n",
    "p.line(ex_df.index, ex_df[str(comp_list[0])],line_color=\"purple\", line_width = 3)\n",
    "p.line(ex_df.index, ex_df[str(comp_list[1])],line_color=\"purple\", line_width = 3)\n",
    "\n",
    "p.xaxis.formatter=DatetimeTickFormatter(\n",
    "\n",
    "        days=[\"%d %B %Y\"],\n",
    "        months=[\"%d %B %Y\"],\n",
    "        years=[\"%d %B %Y\"],\n",
    "    )\n",
    "# output_file(\"line_chart.html\", title=\"Line Chart\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the results from the caches files\n",
    "### These functions let you get the occurences for a given country (cached only) by specifying \n",
    "### either a particular date or a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import os  \n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cause_by_date(country, timestamp):\n",
    "#theloume max to mhna    \n",
    "    time_max = []\n",
    "    timestamp_df = pd.DataFrame(columns=('timestamp', 'occurences', 'reason'))\n",
    "    \n",
    "    for name in glob.glob('Data/trends/'+country+'*'):\n",
    "        country_term = pd.read_pickle(name)\n",
    "        if not country_term.empty:\n",
    "            #print(name)\n",
    "            \n",
    "            spike_cols = [col for col in country_term.columns if country in col]\n",
    "            cc = country_term.loc[timestamp][spike_cols]\n",
    "            #print(cc[0])\n",
    "            timestamp_df = timestamp_df.append({'timestamp': timestamp, 'occurences': cc[0], 'reason': spike_cols[0]}, ignore_index=True)\n",
    "            #print(str(spike_cols[0].split()[1]))\n",
    "    return(timestamp_df.max())\n",
    "#edef\n",
    "\n",
    "def get_cause_by_month(country, timestamp):\n",
    "    timestamp2_df = pd.DataFrame(columns=('timestamp', 'occurences', 'reason'))\n",
    "    for name in glob.glob('Data/trends/'+country+'*'):\n",
    "        country_term = pd.read_pickle(name)\n",
    "        if not country_term.empty:\n",
    "            df = country_term.groupby(pd.TimeGrouper('M')).max()\n",
    "            #print(df.loc[timestamp])\n",
    "            spike_cols = [col for col in country_term.columns if country in col]\n",
    "            #df = country_term.groupby(pd.Grouper(key='date', freq='60s'))\n",
    "            cc = country_term.loc[timestamp][spike_cols]\n",
    "            #print(cc.iloc[0])\n",
    "            timestamp2_df = timestamp2_df.append({'timestamp': timestamp, 'occurences': cc.iloc[0][0], 'reason': spike_cols[0]}, ignore_index=True)\n",
    "    #print('the max index is: '+str(timestamp2_df.index.mac()))\n",
    "    return(timestamp2_df.ix[timestamp2_df.occurences.idxmax()])\n",
    "\n",
    "def get_cause_by_month_with_window(country, timestamp, window):\n",
    "    timestamp3_df = pd.DataFrame(columns=('timestamp', 'occurences', 'reason'))\n",
    "    for name in glob.glob('Data/trends/'+country+'*'):\n",
    "        country_term2 = pd.read_pickle(name)\n",
    "        if not country_term2.empty:\n",
    "            #this dataframe groups by month\n",
    "            df2 = country_term2.groupby(pd.TimeGrouper('M')).max()\n",
    "            #print(df2)\n",
    "            \n",
    "            #find the indexes of the date index(column) that refers to the monthly timestamp\n",
    "            \n",
    "            #get column name that matches the country name (in order to avoid the reference country)\n",
    "            spike_cols2 = [col for col in country_term2.columns if country in col]\n",
    "            \n",
    "            cc2 = country_term2.loc[timestamp][spike_cols2]\n",
    "            #print(cc2)\n",
    "            timestamp3_df = timestamp3_df.append({'timestamp': timestamp, 'occurences': cc2.iloc[0][0].astype('float'), 'reason': spike_cols2[0]}, ignore_index=True)\n",
    "            #print(timestamp3_df)\n",
    "    #timestamp3_df.occurences = timestamp3_df.occurences.astype('float64')\n",
    "    #max_index = timestamp3_df.occurences.idxmax()\n",
    "    #print(timestamp3_df['occurences'].max())\n",
    "    \n",
    "    max_index = (timestamp3_df.ix[timestamp3_df.occurences.idxmax()].name)\n",
    "    max_value  = timestamp3_df.ix[timestamp3_df.occurences.idxmax()].occurences\n",
    "    #print(timestamp3_df.ix[timestamp3_df.occurences.idxmax()])\n",
    "    if not (max_index-window < 0):\n",
    "        \n",
    "        #go back to all the dataframes and find the one that matches the max and return the time window on that\n",
    "        \n",
    "        max_reason = timestamp3_df.iloc[max_index].reason\n",
    "        max_reason2 = max_reason.replace(\" \", \"_\")\n",
    "        #print(max_reason2)\n",
    "        for name in glob.glob('Data/trends/'+max_reason2+'*'):\n",
    "            country_term3 = pd.read_pickle(name)\n",
    "            #print(country_term3)\n",
    "            if not country_term3.empty:\n",
    "                #print(country_term3)\n",
    "                my_old_index = country_term3.loc[(country_term3[max_reason] == max_value) & (country_term3.index == timestamp)]\n",
    "                \n",
    "                \n",
    "                \n",
    "                #my_index = (country_term3.loc[country_term3[max_reason] == max_value && country_term3.index.match(timestamp)].index)\n",
    "                if not my_old_index.empty:\n",
    "                    #print(my_old_index.index.get_indexer)\n",
    "                    my_new_index = (country_term3.index.get_loc(my_old_index.index.values[0]))\n",
    "                    #print(my_new_index)\n",
    "                    return(country_term3.iloc[my_new_index - window: my_new_index+window])\n",
    "                    #window_stop_row = country_term3[country_term3.index < timestamp].iloc[0]\n",
    "                    #return(window_stop_row)\n",
    "\n",
    "                \n",
    "    \n",
    "    #return(timestamp3_df.iloc[(max_index-window):(max_index+window)])\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_cause_by_date('Egypt', '2014-01-01'))\n",
    "#print(get_cause_by_month('Greece', '2008-07'))\n",
    "print(get_cause_by_month_with_window('France', '2017-12', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix the reason \n",
    "### when asked return the occurences around a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TWB import trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TWB.trends.Trends object at 0x12bced470>\n"
     ]
    }
   ],
   "source": [
    "print(trends.Trends('France', '2017-12', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
